{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of assignment_7.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WulCNNkOXOTl",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 7\n",
        "\n",
        "Train a Transformer model for Machine Translation from Russian to English.  \n",
        "Dataset: http://data.statmt.org/wmt18/translation-task/training-parallel-nc-v13.tgz   \n",
        "Make all source and target text to lower case.  \n",
        "Use following tokenization for english:  \n",
        "```\n",
        "import sentencepiece as spm\n",
        "\n",
        "...\n",
        "spm.SentencePieceTrainer.Train('--input=data/text.en --model_prefix=bpe_en --vocab_size=32000 --character_coverage=0.98 --model_type=bpe')\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load('bpe_en.model')\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "...\n",
        "TGT.build_vocab(..., min_freq=5)\n",
        "...\n",
        "\n",
        "```\n",
        "Score: corpus-bleu `nltk.translate.bleu_score.corpus_bleu`  \n",
        "Use last 1000 sentences for model evalutation (test dataset).  \n",
        "Use your target sequence tokenization for BLEU score.  \n",
        "Use max_len=50 for sequence prediction.  \n",
        "\n",
        "\n",
        "Hint: You may consider much smaller model, than shown in the example.  \n",
        "\n",
        "Baselines:  \n",
        "[4 point] BLEU = 0.05  \n",
        "[6 point] BLEU = 0.10  \n",
        "[9 point] BLEU = 0.15  \n",
        "\n",
        "[1 point] Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "\n",
        "\n",
        "Readings:\n",
        "1. BLUE score how to https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
        "1. Transformer code and comments http://nlp.seas.harvard.edu/2018/04/03/attention.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrvShRVMXOTo",
        "colab_type": "code",
        "outputId": "c696e8bf-48a2-45f9-c0f7-1d064ff2161f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "from torchtext import datasets, data\n",
        "!pip install sentencepiece\n",
        "import sentencepiece as spm\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import os\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.85)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd-DWrwKYkz5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    A standard Encoder-Decoder architecture. Base for this and many \n",
        "    other models.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.generator = generator\n",
        "        \n",
        "    def forward(self, batch):\n",
        "        src, tgt = batch.src, batch.tgt\n",
        "        src_mask, tgt_mask = batch.src_mask, batch.tgt_mask\n",
        "        \"Take in and process masked src and target sequences.\"\n",
        "        return self.decode(tgt, tgt_mask, self.encode(src, src_mask), src_mask)\n",
        "    \n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)\n",
        "    \n",
        "    def decode(self, tgt, tgt_mask, memory, src_mask):\n",
        "        x = self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
        "        x = self.generator(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def clones(module, N):\n",
        "    \"Produce N identical layers.\"\n",
        "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"Core encoder is a stack of N layers\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "        \"Pass the input (and mask) through each layer in turn.\"\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        \"Follow Figure 1 (left) for connections.\"\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward)\n",
        "\n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    \"Generic N layer decoder with masking.\"\n",
        "    def __init__(self, layer, N):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.size = size\n",
        "        self.self_attn = self_attn\n",
        "        self.src_attn = src_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        " \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "        \"Follow Figure 1 (right) for connections.\"\n",
        "        m = memory\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
        "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "        return self.sublayer[2](x, self.feed_forward)\n",
        "\n",
        "\n",
        "def subsequent_mask(size):\n",
        "    \"Mask out subsequent positions.\"\n",
        "    attn_shape = (1, size, size)\n",
        "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
        "    return torch.from_numpy(subsequent_mask) == 0\n",
        "\n",
        "\n",
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "    \"Compute 'Scaled Dot Product Attention'\"\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
        "             / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    p_attn = F.softmax(scores, dim = -1)\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "    return torch.matmul(p_attn, value), p_attn\n",
        "\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "        \"Take in model size and number of heads.\"\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_model % h == 0\n",
        "        # We assume d_v always equals d_k\n",
        "        self.d_k = d_model // h\n",
        "        self.h = h\n",
        "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "        self.attn = None\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        \"Implements Figure 2\"\n",
        "        if mask is not None:\n",
        "            # Same mask applied to all h heads.\n",
        "            mask = mask.unsqueeze(1)\n",
        "        nbatches = query.size(0)\n",
        "        \n",
        "        # 1) Do all the linear projections in batch from d_model => h x d_k \n",
        "        query, key, value = \\\n",
        "            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "             for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "        # 2) Apply attention on all the projected vectors in batch. \n",
        "        x, self.attn = attention(query, key, value, mask=mask, \n",
        "                                 dropout=self.dropout)\n",
        "        \n",
        "        # 3) \"Concat\" using a view and apply a final linear. \n",
        "        x = x.transpose(1, 2).contiguous() \\\n",
        "             .view(nbatches, -1, self.h * self.d_k)\n",
        "        return self.linears[-1](x)\n",
        "\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * np.sqrt(self.d_model)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)].clone().detach()\n",
        "        return self.dropout(x)\n",
        "    \n",
        "    \n",
        "def make_model(src_vocab, tgt_vocab, N=6, \n",
        "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "    \"Helper: Construct a model from hyperparameters.\"\n",
        "    c = copy.deepcopy\n",
        "    attn = MultiHeadedAttention(h, d_model)\n",
        "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
        "    position = PositionalEncoding(d_model, dropout)\n",
        "    model = EncoderDecoder(\n",
        "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
        "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                             c(ff), dropout), N),\n",
        "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
        "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
        "        nn.Linear(d_model, tgt_vocab)\n",
        "    )\n",
        "    \n",
        "    # This was important from their code. \n",
        "    # Initialize parameters with Glorot / fan_avg.\n",
        "    for p in model.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "    return model\n",
        "\n",
        "\n",
        "class Batch:\n",
        "    \"Object for holding a batch of data with mask during training.\"\n",
        "    def __init__(self, src, tgt=None, pad=0):\n",
        "        self.src = src\n",
        "        self.src_mask = (src != pad).unsqueeze(-2)\n",
        "        if tgt is not None:\n",
        "            self.tgt = tgt[:, :-1]\n",
        "            self.tgt_y = tgt[:, 1:]\n",
        "            self.tgt_mask = self.make_std_mask(self.tgt, pad)\n",
        "    \n",
        "    @staticmethod\n",
        "    def make_std_mask(tgt, pad):\n",
        "        \"Create a mask to hide padding and future words.\"\n",
        "        tgt_mask = (tgt != pad).unsqueeze(-2)\n",
        "        tgt_mask = tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data).detach()\n",
        "        return tgt_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNqJnp8rc3ti",
        "colab_type": "code",
        "outputId": "a3af0ff3-932b-4395-d9ee-14f2141f8b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GdUWjW5fdycL",
        "colab_type": "code",
        "outputId": "4e97f63e-1549-4a0a-d9a5-a1ffa4e6fe00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ln -s \"/content/drive/My Drive/\" /content/mydrive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ln: failed to create symbolic link '/content/mydrive/My Drive': Operation not supported\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7wMd1KbiPBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "START_PATH = '/content/mydrive'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj5HawWcXOTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize english\n",
        "\n",
        "with open(os.path.join(START_PATH, 'data', 'news-commentary-v13.ru-en.en')) as f:\n",
        "    with open(os.path.join(START_PATH, 'data', 'text.en'), 'w') as out:\n",
        "        out.write(f.read().lower())\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f'--input={os.path.join(START_PATH, \"data\", \"text.en\")} \\\n",
        "    --model_prefix={os.path.join(START_PATH, \"data\", \"bpe_en\")} \\\n",
        "    --vocab_size=32000 --character_coverage=0.98 --model_type=bpe'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "v1ml4LSBXOT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenize russian\n",
        "\n",
        "with open(os.path.join(START_PATH, 'data', 'news-commentary-v13.ru-en.ru')) as f:\n",
        "    with open(os.path.join(START_PATH, 'data', 'text.ru'), 'w') as out:\n",
        "        out.write(f.read().lower())\n",
        "\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    f'--input={os.path.join(START_PATH, \"data\", \"text.ru\")} \\\n",
        "    --model_prefix={os.path.join(START_PATH, \"data\", \"bpe_ru\")} \\\n",
        "    --vocab_size=32000 --character_coverage=0.98 --model_type=bpe'\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxz_Fzd1XOT6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tok_ru = spm.SentencePieceProcessor()\n",
        "tok_ru.load(os.path.join(START_PATH, 'data', 'bpe_ru.model'))\n",
        "\n",
        "tok_en = spm.SentencePieceProcessor()\n",
        "tok_en.load(os.path.join(START_PATH, 'data', 'bpe_en.model'))\n",
        "\n",
        "SRC = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_ru.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "TGT = data.Field(\n",
        "    fix_length=50,\n",
        "    init_token='<s>',\n",
        "    eos_token='</s>',\n",
        "    lower=True,\n",
        "    tokenize = lambda x: tok_en.encode_as_pieces(x),\n",
        "    batch_first=True,\n",
        ")\n",
        "\n",
        "fields = (('src', SRC), ('tgt', TGT))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1lsC8c8XOUA",
        "colab_type": "code",
        "outputId": "6204a6d4-e501-41e3-f394-086c82599baa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "with open(os.path.join(START_PATH, 'data', 'text.ru')) as f:\n",
        "    src_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "with open(os.path.join(START_PATH, 'data', 'text.en')) as f:\n",
        "    tgt_snt = list(map(str.strip, f.readlines()))\n",
        "    \n",
        "examples = [data.Example.fromlist(x, fields) for x in tqdm(zip(src_snt, tgt_snt))]\n",
        "test = data.Dataset(examples[-1000:], fields)\n",
        "train, valid = data.Dataset(examples[:-1000], fields).split(0.9)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "235159it [01:01, 3804.81it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O0_nWmmmHR3",
        "colab_type": "code",
        "outputId": "4ea8ca83-884e-4695-e476-bab83a9e2760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "print('src: ' + \" \".join(train.examples[100].src))\n",
        "print('tgt: ' + \" \".join(train.examples[100].tgt))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "src: ▁лондон . ▁американское ▁чувство ▁ “ исключ ительности ” , ▁особенно ▁когда ▁оно ▁превышает ▁пределы , ▁похоже ▁на ▁цунами , ▁которого ▁следует ▁избегать .\n",
            "tgt: ▁london ▁ – ▁american ▁e x ceptionalism , ▁when ▁it ▁runs ▁rampant , ▁is ▁a ▁tsunami ▁to ▁be ▁avoided .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1HuAmOGmf3F",
        "colab_type": "code",
        "outputId": "602ddf38-8676-4a36-a69c-973eff4c11d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train), len(valid), len(test)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(210743, 23416, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2S5dDApXOUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TGT.build_vocab(train, min_freq=5)\n",
        "SRC.build_vocab(train, min_freq=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpwbNlVJXOUW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BucketIteratorWrapper(DataLoader):\n",
        "    __initialized = False\n",
        "\n",
        "    def __init__(self, iterator: data.Iterator):\n",
        "        # super(BucketIteratorWrapper,self).__init__()\n",
        "        self.batch_size = iterator.batch_size\n",
        "        self.num_workers = -1\n",
        "        self.collate_fn = None\n",
        "        self.pin_memory = False\n",
        "        self.drop_last = False\n",
        "        self.timeout = 0\n",
        "        self.worker_init_fn = None\n",
        "        self.sampler = iterator\n",
        "        self.batch_sampler = iterator\n",
        "        self.__initialized = True\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(\n",
        "            lambda batch: Batch(batch.src, batch.tgt, pad=TGT.vocab.stoi['<pad>']),\n",
        "            self.batch_sampler.__iter__()\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler)\n",
        "\n",
        "\n",
        "class MyCriterion(nn.Module):\n",
        "    def __init__(self, pad_idx):\n",
        "        super(MyCriterion, self).__init__()\n",
        "        self.pad_idx = pad_idx\n",
        "        self.criterion = nn.CrossEntropyLoss(reduction='sum', ignore_index=pad_idx)\n",
        "        \n",
        "    def forward(self, x, target):\n",
        "        x = x.contiguous().permute(0,2,1)\n",
        "        ntokens = (target != self.pad_idx).data.sum()\n",
        "\n",
        "        return self.criterion(x, target) / ntokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p151GMlCrX6q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perhaps_convert_float(param, total):\n",
        "    if isinstance(param, float):\n",
        "        param = int(param * total)\n",
        "    return param\n",
        "\n",
        "\n",
        "class WarmupMultiStepLR(torch.optim.lr_scheduler._LRScheduler):\n",
        "    \"\"\"\n",
        "    Learning rate scheduler with exponential warmup and step decay.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, iterations, warmup_steps=0,\n",
        "                 remain_steps=1.0, decay_interval=None, decay_steps=4,\n",
        "                 decay_factor=0.5, last_epoch=-1):\n",
        "        \"\"\"\n",
        "        Constructor of WarmupMultiStepLR.\n",
        "        Parameters: warmup_steps, remain_steps and decay_interval accept both\n",
        "        integers and floats as an input. Integer input is interpreted as\n",
        "        absolute index of iteration, float input is interpreted as a fraction\n",
        "        of total training iterations (epochs * steps_per_epoch).\n",
        "        If decay_interval is None then the decay will happen at regulary spaced\n",
        "        intervals ('decay_steps' decays between iteration indices\n",
        "        'remain_steps' and 'iterations').\n",
        "        :param optimizer: instance of optimizer\n",
        "        :param iterations: total number of training iterations\n",
        "        :param warmup_steps: number of warmup iterations\n",
        "        :param remain_steps: start decay at 'remain_steps' iteration\n",
        "        :param decay_interval: interval between LR decay steps\n",
        "        :param decay_steps: max number of decay steps\n",
        "        :param decay_factor: decay factor\n",
        "        :param last_epoch: the index of last iteration\n",
        "        \"\"\"\n",
        "\n",
        "        # iterations before learning rate reaches base LR\n",
        "        self.warmup_steps = perhaps_convert_float(warmup_steps, iterations)\n",
        "        logging.info(f'Scheduler warmup steps: {self.warmup_steps}')\n",
        "\n",
        "        # iteration at which decay starts\n",
        "        self.remain_steps = perhaps_convert_float(remain_steps, iterations)\n",
        "        logging.info(f'Scheduler remain steps: {self.remain_steps}')\n",
        "\n",
        "        # number of steps between each decay\n",
        "        if decay_interval is None:\n",
        "            # decay at regulary spaced intervals\n",
        "            decay_iterations = iterations - self.remain_steps\n",
        "            self.decay_interval = decay_iterations // (decay_steps)\n",
        "            self.decay_interval = max(self.decay_interval, 1)\n",
        "        else:\n",
        "            self.decay_interval = perhaps_convert_float(decay_interval,\n",
        "                                                        iterations)\n",
        "        logging.info(f'Scheduler decay interval: {self.decay_interval}')\n",
        "\n",
        "        # multiplicative decay factor\n",
        "        self.decay_factor = decay_factor\n",
        "        logging.info(f'Scheduler decay factor: {self.decay_factor}')\n",
        "\n",
        "        # max number of decay steps\n",
        "        self.decay_steps = decay_steps\n",
        "        logging.info(f'Scheduler max decay steps: {self.decay_steps}')\n",
        "\n",
        "        if self.warmup_steps > self.remain_steps:\n",
        "            logging.warn(f'warmup_steps should not be larger than '\n",
        "                         f'remain_steps, setting warmup_steps=remain_steps')\n",
        "            self.warmup_steps = self.remain_steps\n",
        "\n",
        "        super(WarmupMultiStepLR, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "    def get_lr(self):\n",
        "        if self.last_epoch <= self.warmup_steps:\n",
        "            # exponential lr warmup\n",
        "            if self.warmup_steps != 0:\n",
        "                warmup_factor = math.exp(math.log(0.01) / self.warmup_steps)\n",
        "            else:\n",
        "                warmup_factor = 1.0\n",
        "            inv_decay = warmup_factor ** (self.warmup_steps - self.last_epoch)\n",
        "            lr = [base_lr * inv_decay for base_lr in self.base_lrs]\n",
        "\n",
        "        elif self.last_epoch >= self.remain_steps:\n",
        "            # step decay\n",
        "            decay_iter = self.last_epoch - self.remain_steps\n",
        "            num_decay_steps = decay_iter // self.decay_interval + 1\n",
        "            num_decay_steps = min(num_decay_steps, self.decay_steps)\n",
        "            lr = [\n",
        "                base_lr * (self.decay_factor ** num_decay_steps)\n",
        "                for base_lr in self.base_lrs\n",
        "                ]\n",
        "        else:\n",
        "            # base lr\n",
        "            lr = [base_lr for base_lr in self.base_lrs]\n",
        "        return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dvxbjg9zXOUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "num_epochs = 10\n",
        "\n",
        "train_iter, valid_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train, valid, test),\n",
        "    batch_sizes=(batch_size, batch_size, batch_size),\n",
        "    sort_key=lambda x: len(x.src),\n",
        "    shuffle=True,\n",
        "    device=DEVICE,\n",
        "    sort_within_batch=False\n",
        ")\n",
        "\n",
        "train_iter = BucketIteratorWrapper(train_iter)\n",
        "valid_iter = BucketIteratorWrapper(valid_iter)\n",
        "test_iter = BucketIteratorWrapper(test_iter)\n",
        "\n",
        "model = make_model(len(SRC.vocab), len(TGT.vocab))\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "pad_idx = TGT.vocab.stoi['<pad>']\n",
        "criterion = MyCriterion(pad_idx=pad_idx)\n",
        "criterion = criterion.to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "steps = 25\n",
        "scheduler = WarmupMultiStepLR(optimizer, steps)\n",
        "\n",
        "# Share weights between target embeddings and output dense layer. Notice, they have the same shape.\n",
        "# model.src_embed[0].lut.weight = model.tgt_embed[0].lut.weight\n",
        "model.generator.weight = model.tgt_embed[0].lut.weight"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgxh0fxzH3KZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()\n",
        "for instance in list(tqdm._instances): \n",
        "    tqdm._decr_instances(instance)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USPN5730XOUi",
        "colab_type": "code",
        "outputId": "469ef323-e8ce-4b39-a013-babee87d71b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def train_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(batch)\n",
        "        loss = criterion(pred, batch.tgt_y)\n",
        "        loss.backward()\n",
        "        curr_loss = loss.data.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        data_iter.set_postfix(loss=curr_loss)\n",
        "        counter +=1\n",
        "\n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "def valid_epoch(data_iter, model, criterion):\n",
        "    total_loss = 0\n",
        "    data_iter = tqdm(data_iter)\n",
        "    counter = 0\n",
        "    for batch in data_iter:\n",
        "        pred = model(batch)\n",
        "        curr_loss = criterion(pred, batch.tgt_y).data.item()\n",
        "\n",
        "        total_loss += curr_loss\n",
        "        data_iter.set_postfix(loss=curr_loss)\n",
        "        counter += 1\n",
        "\n",
        "    total_loss /= counter\n",
        "    return total_loss\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    loss = train_epoch(train_iter, model, criterion)\n",
        "    print(f'\\ntrain: {loss}\\n')\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        loss = valid_epoch(valid_iter, model, criterion)\n",
        "        scheduler.step(loss)\n",
        "        print(f'\\nvalid: {loss}\\n')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:56<00:00,  1.72it/s, loss=4.52]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 5.187718317880729\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.21it/s, loss=4.96]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 4.3222730993573135\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:53<00:00,  1.73it/s, loss=3.82]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 4.138006118549748\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.20it/s, loss=4.64]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 3.6707735296155586\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:58<00:00,  1.74it/s, loss=3.42]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 3.6640484396732425\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.20it/s, loss=4.43]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 3.31609748621456\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:55<00:00,  1.73it/s, loss=3.37]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 3.368395047399154\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.20it/s, loss=4.31]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 3.119629490570944\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:51<00:00,  1.74it/s, loss=2.95]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 3.1540818428660278\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.22it/s, loss=4.12]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 2.919658299352302\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:50<00:00,  1.73it/s, loss=2.87]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 2.9456366962856717\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.19it/s, loss=3.93]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 2.7581621030640733\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:51<00:00,  1.73it/s, loss=2.71]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 2.7891828085917303\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.24it/s, loss=3.81]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 2.6518022681845994\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:52<00:00,  1.73it/s, loss=2.74]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 2.6559777901110393\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.23it/s, loss=3.74]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 2.5640838328606446\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:50<00:00,  2.04it/s, loss=2.59]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 2.5374351459774163\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.21it/s, loss=3.66]\n",
            "  0%|          | 0/1647 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 2.488338206635147\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1647/1647 [15:49<00:00,  1.73it/s, loss=2.39]\n",
            "  0%|          | 0/183 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "train: 2.432182786677487\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 183/183 [00:35<00:00,  5.20it/s, loss=3.59]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "valid: 2.4265884428076405\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alIo0uPVXOUy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypotheses = []\n",
        "references = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in test_iter:\n",
        "        pred = model(batch)\n",
        "        sents = torch.argmax(torch.softmax(pred, dim=-1), dim=-1)\n",
        "        hypotheses.extend([[TGT.vocab.itos[ix] for ix in sent] for sent in sents])\n",
        "        references.extend([[[TGT.vocab.itos[ix] for ix in sent]] for sent in batch.tgt_y])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ1k5JqMXOU2",
        "colab_type": "code",
        "outputId": "feb8c71c-c61c-4d13-b5cd-1424ef925af3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus_bleu(\n",
        "    references, hypotheses, smoothing_function=SmoothingFunction().method3,\n",
        "    auto_reweigh=True\n",
        ")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1531192190510838"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmkvaOyWXOUk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start_token = len(TGT.vocab)\n",
        "# end_token = start_token + 1\n",
        "\n",
        "# def beam_search(model, src, src_mask, max_len=50, k=5, tau=1):\n",
        "#     \"\"\"\n",
        "#     Generate sequence in target language from NMT model *model* conditioned on input sequence *src* in source language\n",
        "#     model: NMT model\n",
        "#     src: batch sequence of token ids in source language\n",
        "#     src_mask: batch input sequence *src* mask\n",
        "#     max_len: max generated sequence length (50 by default: \"Use max_len=50 for sequence prediction.\")\n",
        "#     k: size of beam\n",
        "#     tau: temperature\n",
        "#     \"\"\"\n",
        "\n",
        "#     pred = model.encode(src, src_mask)\n",
        "#     # ...\n",
        "#     # beam = [([start_token], 0)]  # 0 as log(1) == 0\n",
        "\n",
        "#     # for i in range(max_len):\n",
        "#     #     candidates = []\n",
        "#     #     candidates_proba = []\n",
        "#     #     for snt, snt_proba in beam:\n",
        "#     #         if snt[-1] == end_token:\n",
        "#     #             candidates.append(snt)\n",
        "#     #             candidates_proba.append(snt_proba)\n",
        "#     #         else:\n",
        "#     #             # probability vector of the next token\n",
        "#     #             model.encode(src, src_mask)\n",
        "#     #             if len(snt) == 1:\n",
        "#     #                 proba = lm.infer(start_token, snt[-1], tau)\n",
        "#     #             else:\n",
        "#     #                 proba = lm.infer(*snt[-2:], tau)\n",
        "\n",
        "#     #             # top-k most probable (token ids, token proba) pairs\n",
        "#     #             best_k_pairs = sorted(list(enumerate(proba)), key=lambda elem: elem[1], reverse=True)[:k]\n",
        "\n",
        "#     #             # TODO update candidates' sequences and corresponding probabilities\n",
        "#     #             for token_id, token_proba in best_k_pairs:\n",
        "#     #                 candidates.append(snt + [token_id])\n",
        "#     #                 candidates_proba.append(snt_proba + math.log(token_proba))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "atoVE7qmXOUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.eval()\n",
        "# with torch.no_grad():\n",
        "#     for i, batch in enumerate(valid_iter):\n",
        "#         src = batch.src[:1]\n",
        "#         src_key_padding_mask = src != SRC.vocab.stoi[\"<pad>\"]\n",
        "#         beam = beam_search(model, src, src_key_padding_mask)\n",
        "        \n",
        "#         seq = []\n",
        "#         for i in range(1, src.size(1)):\n",
        "#             sym = SRC.vocab.itos[src[0, i]]\n",
        "#             if sym == \"</s>\": break\n",
        "#             seq.append(sym)\n",
        "#         seq = tok_ru.decode_pieces(seq)\n",
        "#         print(\"\\nSource:\", seq)\n",
        "        \n",
        "#         print(\"Translation:\")\n",
        "#         for pred, pred_proba in beam:                \n",
        "#             seq = []\n",
        "#             for i in range(1, pred.size(1)):\n",
        "#                 sym = TGT.vocab.itos[pred[0, i]]\n",
        "#                 if sym == \"</s>\": break\n",
        "#                 seq.append(sym)\n",
        "#             seq = tok_en.decode_pieces(seq)\n",
        "#             print(f\"pred {pred_proba:.2f}:\", seq)\n",
        "\n",
        "#         seq = []\n",
        "#         for i in range(1, batch.tgt.size(1)):\n",
        "#             sym = TGT.vocab.itos[batch.tgt[0, i]]\n",
        "#             if sym == \"</s>\": break\n",
        "#             seq.append(sym)\n",
        "#         seq = tok_en.decode_pieces(seq)\n",
        "#         print(\"Target:\", seq)\n",
        "#         break"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}