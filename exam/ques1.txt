3. Language model inference methods: properties, differences, cases of usage (without formulas, at least 2)  

Inference methods -- это способы получения из обученной модели последовательности объектов (токенов/предложений и т.д.). Например, обученная языковая модель (language model) возвращает вероятности следующих объектов. Мы можем с ними работать, как нам будет это удобно.

Существует как минимум 3 inference метода:
1) argmax, когда мы берём самый вероятный на данном шаге следующий токен (берём элемент, на котором достигается максимум из вывода софтмакса). В результате этого метода возможны зацикливания, вывод чаще всего тривиален. Возможен для использования для предварительной оценки качества языковой модели.
2) sampling, когда мы в соответствии с выводом софтмакса (вектором вероятностей) берём следующий токен случайным образом, учитывая веротяности каждого токена, т.е. у наиболее вероятного токена самая большая вероятность быть выбранным следующим токеном, но это не гарантируется, может быть выбран и другой элемент (см. функцию random.sample). Более оригинален, чем софтмакс, но нельзя контролировать "рандомность" вывода (мы можем захотеть больше или меньше разнообразия). Чтобы решить последнюю проблему, мы можем добавить температуру (temperature) и перед софтмаксом на выходе из линейного слоя дополнительно сгладить или усилить различия, разделив вектор вывода на температуру. При меньшей температуре результат упрощается, вывод более тривиален, поскольку различие между элементами увеличивается, т.е. вероятность бывших наиболее вероятными токенов увеличивается по сравнению с остальными (предельный случай при t = 0, когда всё одинаково). Напротив, при большей температуре больше разнообразия, т.к. мы делаем выводы более равновероятными, если так можно выразиться.
3) beam search (лучевой поиск), когда мы сохраняем top-k последовательностей на каждом шаге. Плюс в том, что учитывается не один вариант, а несколько, и поэтому несмотря на жадный алгоритм, это добавляет рандомности на выводе. Оптимальное решение не гарантировано, но по крайней мере в этом случае мы учитываем случаи, когда предложение не очень вероятно, но возможно. Т.к. мы храним только k вариантов, то мы оптимизируем по памяти, учить весь breadth-firsth search было бы слишком жирно, т.к. возможных токенок очень много. А так мы балансируем в плоскости рандома и затрат по памяти и времени работы.
