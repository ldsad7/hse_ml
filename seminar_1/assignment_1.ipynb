{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1\n",
    "\n",
    "Using text http://www.gutenberg.org/files/2600/2600-0.txt\n",
    "1. Make text lowercase and remove all punctuation except spaces and dots.\n",
    "2. Tokenize text by BPE with vocab_size = 100\n",
    "3. Train 3-gram language model with laplace smoothing $\\delta=1$\n",
    "4. Using beam search with k=10 generate sequences of length=10 conditioned on provided inputs. Treat dots as terminal tokens.\n",
    "5. Calculate perplexity of the language model for the first sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3227578"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = open('peace.txt', 'r', encoding='utf-8-sig').read()[2:]\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’“:”-(@,*;#—%[?=‘!)]/\n"
     ]
    }
   ],
   "source": [
    "print(''.join(set(''.join(set(text.lower()) - set(string.ascii_lowercase) - set(string.digits) - set(' $.\\nëóýêíáèïéîüúçâœæôöäà')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # TODO\n",
    "    # make lowercase\n",
    "    # replace all punctuation except dots with spaces\n",
    "    # collapse multiple spaces into one '   ' -> ' '\n",
    "    text = text.lower()\n",
    "    text = re.sub('[@;*—/?!()“”‘’=#:,[\\]%-]', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = preprocess_text(text)\n",
    "assert len(text) == 3141169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text.split('.')\n",
    "texts = [text.strip() for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import nltk\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "\n",
    "class BPE(TransformerMixin):\n",
    "    def __init__(self, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        # index to token\n",
    "        self.itos = []\n",
    "        # token to index\n",
    "        self.stoi = {}\n",
    "        \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        fit itos and stoi\n",
    "        texts: list of strings \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO\n",
    "        # tokenize text by symbols and fill in self.itos and self.stoi\n",
    "        unique_symbols = set()\n",
    "        for text in texts:\n",
    "            unique_symbols |= set(text)\n",
    "        self.itos = list(unique_symbols)\n",
    "        self.stoi = {token: index for index, token in enumerate(self.itos)}\n",
    "        texts = self.transform(texts)\n",
    "\n",
    "        while len(self.itos) < self.vocab_size:\n",
    "            # TODO\n",
    "            # count bigram freqencies in the text\n",
    "            bigrams_counter = Counter()\n",
    "            for text in texts:\n",
    "                # bigrams_counter.update(zip(*(text[i:] for i in range(2))))\n",
    "                # or simply\n",
    "                bigrams_counter.update(nltk.bigrams(text))\n",
    "            new_token_numbers = bigrams_counter.most_common(1)[0][0]\n",
    "            new_token_letters = self.decode(new_token_numbers)  # most common bigram in the text\n",
    "            new_id = len(self.itos)\n",
    "\n",
    "            self.itos.append(new_token_letters)\n",
    "            self.stoi[new_token_letters] = new_id\n",
    "\n",
    "            # find occurences of the new_token_numbers in the text and replace them with new_id\n",
    "            for i, text in enumerate(texts):\n",
    "                # TODO\n",
    "                indices_to_be_removed = []\n",
    "                for j in range(1, len(text)):\n",
    "                    if (text[j - 1], text[j]) == new_token_numbers:\n",
    "                        text[j] = new_id\n",
    "                        indices_to_be_removed.append(j - 1)\n",
    "                texts[i] = [token for j, token in enumerate(text) if j not in indices_to_be_removed]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def recursive_replace(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "        for token in self.itos[::-1]:\n",
    "            if len(token) > len(text):\n",
    "                continue\n",
    "            if token in text:\n",
    "                index = text.index(token)\n",
    "                return self.recursive_replace(text[:index]) + [self.stoi[token]] + \\\n",
    "                       self.recursive_replace(text[index + len(token):])\n",
    "        raise ValueError(\"Impossible to transform the text. Maybe you sent different texts to the fit and transform methods.\")\n",
    "\n",
    "    def transform(self, texts):\n",
    "        \"\"\"\n",
    "        convert texts to a sequences of token ids\n",
    "        texts: list of strings\n",
    "        \"\"\"\n",
    "        # TODO tokenize text by symbols using self.stoi\n",
    "        texts = [self.recursive_replace(text) for text in texts]\n",
    "        return texts\n",
    "\n",
    "    def decode_token(self, tok):\n",
    "        \"\"\"\n",
    "        tok: int or tuple\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        return self.itos[tok]\n",
    "\n",
    "    def decode(self, text):\n",
    "        \"\"\"\n",
    "        convert token ids into text\n",
    "        \"\"\"\n",
    "        return ''.join(map(self.decode_token, text))\n",
    "        \n",
    "        \n",
    "vocab_size = 100\n",
    "bpe = BPE(vocab_size)\n",
    "tokenized_texts = bpe.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert bpe.decode(tokenized_texts[0]) == texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "for i in range(50):\n",
    "    n = random.randint(0, len(texts) - 1)\n",
    "    assert bpe.decode(tokenized_texts[n]) == texts[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "start_token = vocab_size\n",
    "end_token = vocab_size + 1\n",
    "\n",
    "\n",
    "class LM:\n",
    "    def __init__(self, vocab_size, delta=1):\n",
    "        self.delta = delta\n",
    "        self.vocab_size = vocab_size + 2\n",
    "        # TODO create array for storing 3-gram counters\n",
    "        self.proba = []\n",
    "\n",
    "    def infer(self, a, b, tau=1):\n",
    "        \"\"\"\n",
    "        return vector of probabilities of size self.vocab for 3-grams which start with (a,b) tokens\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        assert tau != 0\n",
    "        result = [self.get_proba(a, b, c, tau) for c in range(self.vocab_size)]\n",
    "        return result\n",
    "\n",
    "    def get_proba(self, a, b, c, tau=1):\n",
    "        \"\"\"\n",
    "        get probability of 3-gram (a,b,c)\n",
    "        a: first token id\n",
    "        b: second token id\n",
    "        c: third token id\n",
    "        tau: temperature\n",
    "        \"\"\"\n",
    "        # TODO approximate probability by counters\n",
    "        assert tau != 0\n",
    "        result = pow(self.delta + self.proba[(a, b, c)], 1 / tau) / sum(\n",
    "            [pow(self.delta + self.proba[(a, b, x)], 1 / tau) for x in range(self.vocab_size)])\n",
    "        return result\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        \"\"\"\n",
    "        train language model on texts\n",
    "        texts: list of lists\n",
    "        \"\"\"\n",
    "        # TODO count 3-grams in the texts\n",
    "        trigrams_counter = Counter()\n",
    "        for text in texts:\n",
    "            # trigrams_counter.update(zip(*(text[i:] for i in range(3))))\n",
    "            # or simply\n",
    "            trigrams_counter.update(nltk.trigrams([start_token] + text + [end_token]))\n",
    "        self.proba = trigrams_counter\n",
    "        return self\n",
    "\n",
    "lm = LM(vocab_size, 1).fit(tokenized_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import math\n",
    "\n",
    "def beam_search(input_seq, lm, max_len=10, k=5, tau=1):\n",
    "    \"\"\"\n",
    "    generate sequence from language model *lm* conditioned on input_seq\n",
    "    input_seq: sequence of token ids for conditioning\n",
    "    lm: language model\n",
    "    max_len: max generated sequence length\n",
    "    k: size of beam\n",
    "    tau: temperature\n",
    "    \"\"\"\n",
    "\n",
    "    if not input_seq:\n",
    "        raise ValueError('input_seq is empty')\n",
    "    \n",
    "    # TODO store in beam tuples of current sequences and their log probabilities\n",
    "    beam = [(input_seq, 0)]  # 0 as log(1) == 0\n",
    "\n",
    "    for i in range(max_len):\n",
    "        candidates = []\n",
    "        candidates_proba = []\n",
    "        for snt, snt_proba in beam:\n",
    "            if snt[-1] == end_token:\n",
    "                # TODO process terminal token\n",
    "                # add to candidates as it is\n",
    "                candidates.append(snt)\n",
    "                candidates_proba.append(snt_proba)\n",
    "            else:\n",
    "                # probability vector of the next token\n",
    "                if len(snt) == 1:\n",
    "                    proba = lm.infer(start_token, snt[-1], tau)\n",
    "                else:\n",
    "                    proba = lm.infer(*snt[-2:], tau)\n",
    "\n",
    "                # top-k most probable (token ids, token proba) pairs\n",
    "                best_k_pairs = sorted(list(enumerate(proba)), key=lambda elem: elem[1], reverse=True)[:k]\n",
    "\n",
    "                # TODO update candidates' sequences and corresponding probabilities\n",
    "                for token_id, token_proba in best_k_pairs:\n",
    "                    candidates.append(snt + [token_id])\n",
    "                    candidates_proba.append(snt_proba + math.log(token_proba))\n",
    "\n",
    "        # select top-k most probable sequences from candidates\n",
    "        top_k_candidates = sorted(list(enumerate(candidates_proba)), key=lambda elem: elem[1], reverse=True)[:k]\n",
    "        beam = [(candidates[candidate_index], candidate_proba) for candidate_index, candidate_proba in top_k_candidates]\n",
    "    return beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO print decoded generated strings and their probabilities\n",
    "def print_decoded_sequences(results):\n",
    "    for seq, proba in results:\n",
    "        print(f'genderated string: {bpe.decode(seq)} | log probability: {proba}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# strip end token as bpe doesn't have it in self.itos\n",
    "def strip_end_token(results):\n",
    "    for i, result in enumerate(results):\n",
    "        if result[0][-1] == end_token:\n",
    "            results[i] = (result[0][:-1], result[1])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genderated string: horse of the cound the c | log probability: -1.3536314912661447\n",
      "genderated string: horse of the said not  | log probability: -1.9001351870506915\n",
      "genderated string: horse of the countere  | log probability: -2.292039183018149\n",
      "genderated string: horse of the cound him  | log probability: -2.608492913751813\n",
      "genderated string: horse of the counted to  | log probability: -2.7446600910824483\n",
      "genderated string: horse of the cound the s | log probability: -2.8334885942426555\n",
      "genderated string: horse of the counderstand  | log probability: -3.6449111435014103\n",
      "genderated string: horse and the cound the c | log probability: -3.660157755865623\n",
      "genderated string: horse of the prince and | log probability: -3.7028044294026317\n",
      "genderated string: horse of the french sa | log probability: -3.9261584346778005\n"
     ]
    }
   ],
   "source": [
    "input1 = 'horse '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "results = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "results = strip_end_token(results)\n",
    "print_decoded_sequences(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genderated string: her | log probability: -0.05834838774541152\n",
      "genderated string: hers who had been  | log probability: -4.9607753160300065\n",
      "genderated string: herself said no | log probability: -4.995819366802682\n",
      "genderated string: hers were said n | log probability: -5.023525935156409\n",
      "genderated string: hers | log probability: -5.147449208919879\n",
      "genderated string: herself and the coun | log probability: -5.652776409930521\n",
      "genderated string: hers what is the cou | log probability: -5.739949162261469\n",
      "genderated string: hers what is not b | log probability: -5.90341700199199\n",
      "genderated string: herself they had b | log probability: -6.443309174549449\n",
      "genderated string: her who had been s | log probability: -6.481797449679599\n"
     ]
    }
   ],
   "source": [
    "input1 = 'her'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "results = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "results = strip_end_token(results)\n",
    "print_decoded_sequences(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genderated string: what | log probability: -3.857214768933151\n",
      "genderated string: what would not know | log probability: -10.255435891167032\n",
      "genderated string: what would notion  | log probability: -10.618565066731836\n",
      "genderated string: what would not kne | log probability: -11.028625779400514\n",
      "genderated string: what would not been | log probability: -11.439729787892619\n",
      "genderated string: what would not und | log probability: -11.504062725885246\n",
      "genderated string: what would not up  | log probability: -11.510622312112565\n",
      "genderated string: what would not beg | log probability: -11.77715034132269\n",
      "genderated string: what would notions  | log probability: -11.821996012469603\n",
      "genderated string: what would not the c | log probability: -11.835417210580742\n"
     ]
    }
   ],
   "source": [
    "input1 = 'what'\n",
    "input1 = bpe.transform([input1])[0]\n",
    "results = beam_search(input1, lm, max_len=10, k=10, tau=1)\n",
    "results = strip_end_token(results)\n",
    "print_decoded_sequences(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genderated string: gun and the cound the c | log probability: -2.0479674572284665\n",
      "genderated string: gun been said no | log probability: -2.2351840799068863\n",
      "genderated string: gun t been said n | log probability: -2.4100062859620257\n",
      "genderated string: gun and the said not  | log probability: -2.5944711530130125\n",
      "genderated string: gun t you said no | log probability: -2.82762409839334\n",
      "genderated string: gun and the countere  | log probability: -2.9863751489804704\n",
      "genderated string: gun and the cound him  | log probability: -3.3028288797141347\n",
      "genderated string: gun and the counted to  | log probability: -3.4389960570447697\n",
      "genderated string: gun and the cound the s | log probability: -3.5278245602049774\n",
      "genderated string: gun said not been | log probability: -3.57709182270748\n"
     ]
    }
   ],
   "source": [
    "input1 = 'gun '\n",
    "input1 = bpe.transform([input1])[0]\n",
    "results = beam_search(input1, lm, max_len=10, k=10, tau=0.1)\n",
    "results = strip_end_token(results)\n",
    "print_decoded_sequences(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.041252131484478"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def perplexity(snt, lm):\n",
    "    \"\"\"\n",
    "    snt: sequence of token ids\n",
    "    lm: language model\n",
    "    \"\"\"\n",
    "    # TODO perplexity for the sentence\n",
    "    if not snt:\n",
    "        raise ValueError('snt is empty')\n",
    "    snt = [start_token] + snt + [end_token]\n",
    "    result = pow(2, (-1 / (len(snt) - 2)) * sum(\n",
    "        [\n",
    "            math.log(\n",
    "                lm.get_proba(\n",
    "                    snt[i], token_id, snt[i + 2]\n",
    "                ), 2\n",
    "            ) for i, token_id in enumerate(snt[1:-1])\n",
    "        ]\n",
    "    ))\n",
    "    return result\n",
    "\n",
    "perplexity(tokenized_texts[0], lm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
